<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-08-20T23:22:03+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Daria Zahaleanu</title><subtitle>Academic researcher and professional - exploring ideas and building solutions</subtitle><author><name>Daria Zahaleanu</name></author><entry><title type="html">The MiniCheck Method for Efficient and Verifiable LLM Fact-Checking</title><link href="http://localhost:4000/blog/2025/08/14/a-visual-guide-to-ai-interpretability/" rel="alternate" type="text/html" title="The MiniCheck Method for Efficient and Verifiable LLM Fact-Checking" /><published>2025-08-14T13:00:00+02:00</published><updated>2025-08-14T13:00:00+02:00</updated><id>http://localhost:4000/blog/2025/08/14/a-visual-guide-to-ai-interpretability</id><content type="html" xml:base="http://localhost:4000/blog/2025/08/14/a-visual-guide-to-ai-interpretability/">&lt;p&gt;&lt;strong&gt;Breaking Down Fake News: An Inside Look at the MiniCheck Method&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In an era of increasingly complex and nuanced misinformation, fact-checking a single claim can be a painstaking process. The MiniCheck method is a novel approach that automates this human-like workflow by using a multi-stage, Chain-of-Thought (CoT) reasoning process powered by large language models (LLMs). Rather than simply assigning a label, MiniCheck deconstructs a claim, verifies its components, and then synthesizes a final verdict.&lt;/p&gt;

&lt;p&gt;This methodology demonstrates a practical application of LLMs for high-stakes tasks, showing how they can be orchestrated in a pipeline to produce more transparent and verifiable results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Three Stages of MiniCheck&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Claim Decomposition: The process begins by feeding an original claim to a powerful LLM, which acts as a “decomposer.” The model’s sole task is to break the complex claim into a list of simple, verifiable sub-claims. For example, the claim “A new jobs report shows unemployment is at a 50-year low due to growth in manufacturing” would be broken into two sub-claims:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;“Unemployment has fallen to a 50-year low.”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“The decline is driven by growth in the manufacturing sector.”
This step is critical because it isolates individual facts that can be checked independently.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Sub-Claim Verification: In this stage, a separate model (the “verifier”) analyzes each sub-claim against a provided piece of evidence, such as a news article or official report. Using a specific prompt structure, the verifier determines whether the evidence supports, contradicts, or provides not enough information to verify the sub-claim. Crucially, this stage can be performed in a batch, making it highly efficient.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Verdict Aggregation: The final verdict is an aggregation of the verifier’s results. The rule is simple yet powerful: if even a single sub-claim is found to be Contradicted by the evidence, the entire original claim is flagged as False. This mimics the logical principle that a single factual error can invalidate a larger statement.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Results and Key Insights&lt;/strong&gt;
MiniCheck was benchmarked on the LIAR-PLUS dataset, which contains claims paired with their corresponding justification texts. Using GPT-3.5 Turbo for both the decomposition and verification steps, the method achieved an impressive overall accuracy of 67.5%.&lt;/p&gt;

&lt;p&gt;The classification report provides a deeper understanding of its performance:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;High Precision: The model had a high precision for “False” claims (84%), meaning that when it labeled something as false, it was almost certainly correct.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lower Recall: The recall for false claims was 57%, indicating that some false claims were missed, likely because the verifier could not find a clear contradiction in the evidence and defaulted to “Not Enough Information.”&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The MiniCheck method highlights both the promise and the challenges of using LLMs for complex, multi-step tasks. While the process itself is a major step toward creating more transparent and explainable AI systems, its accuracy is heavily dependent on the capabilities of the underlying LLMs and the quality of the provided evidence.&lt;/p&gt;</content><author><name>Daria Zahaleanu</name></author><category term="AI" /><summary type="html">Breaking Down Fake News: An Inside Look at the MiniCheck Method</summary></entry><entry><title type="html">The NRFE Method for Fake News Detection</title><link href="http://localhost:4000/blog/2025/08/14/the-nrfe-method-for-fake-news-detaction/" rel="alternate" type="text/html" title="The NRFE Method for Fake News Detection" /><published>2025-08-14T13:00:00+02:00</published><updated>2025-08-14T13:00:00+02:00</updated><id>http://localhost:4000/blog/2025/08/14/the-nrfe-method-for-fake-news-detaction</id><content type="html" xml:base="http://localhost:4000/blog/2025/08/14/the-nrfe-method-for-fake-news-detaction/">&lt;h1 id=&quot;beyond-black-boxes-using-explainability-xai-to-build-trustworthy-anomaly-detection-systems&quot;&gt;Beyond Black Boxes: Using Explainability (XAI) to Build Trustworthy Anomaly Detection Systems&lt;/h1&gt;

&lt;!-- 9. Hot topics
    * Agent governance
    * Internal governance -&gt; how do they log stuff? What do they do with the data?
        * No real company experience
        * GovAi - Jonas Schultz, SafeAI as well --&gt;

&lt;p&gt;In the world of AI, “hallucination” is a term for a serious flaw—when a large language model (LLM) generates false or fabricated information with high confidence. But what if this flaw could be turned into a strength? The Negative Reasoning for Fake News Detection (NRFE) method proposes a revolutionary approach: purposefully using LLM hallucinations as a tool to train more robust and accurate fact-checking models.&lt;/p&gt;

&lt;p&gt;This method, developed in a research paper on XAI (Explainable AI), operates on a sophisticated principle: if a model can be taught the difference between sound logic and absurd, fabricated logic, it can become an expert at detecting fake news.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How It Works: A Teacher-Student Paradigm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The NRFE method is built around a two-stage process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The Reasoning Generator: The process begins with a capable LLM that acts as an initial data augmenter. For a given news statement, this LLM is prompted to generate two types of explanations:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Positive Reasoning (R p): A plausible, logical reason why the statement could be true.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Negative Reasoning (R n): A fabricated, illogical, or outright false reason that still sounds convincing. This is where the model’s “hallucination” is deliberately triggered.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The NRFE Model (The Teacher): A custom-built, dual-encoder model is then trained on this augmented dataset. It has two separate encoders—one for the original news statement and one for the generated reasoning. A crucial cross-attention layer forces these two encoders to communicate and learn the semantic consistency (or lack thereof) between the statement and its reasoning. The model’s task is to learn that a true statement aligns with its positive reasoning, while a false statement is more semantically consistent with a negative, illogical reason.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Key Performance Results&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This approach was benchmarked against several fine-tuned LLMs, and the results were striking. While off-the-shelf fine-tuned models achieved accuracy scores ranging from 51% (Llama3) to 59% (Gemma2), the NRFE-D model, a distilled student version of the main NRFE model, significantly outperformed them with an accuracy of 69%.&lt;/p&gt;

&lt;p&gt;The results highlight a powerful insight: by training on AI-generated reasoning, the NRFE model learns to detect the structure of falsehood itself, not just the surface-level keywords or biases in a given statement.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The NRFE method represents a major shift in how we approach AI-powered fact-checking. Instead of treating AI hallucinations as a bug to be fixed, this method repurposes them as a powerful tool to train more resilient and transparent models. While still a research-focused approach, it lays the groundwork for a new generation of AI systems that can explain their decisions and better combat the growing threat of misinformation.&lt;/p&gt;</content><author><name>Daria Zahaleanu</name></author><category term="AI" /><summary type="html">Beyond Black Boxes: Using Explainability (XAI) to Build Trustworthy Anomaly Detection Systems</summary></entry></feed>